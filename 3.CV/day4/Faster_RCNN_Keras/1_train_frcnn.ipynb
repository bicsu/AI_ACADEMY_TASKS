{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras_frcnn import config, data_generators\n",
    "from keras_frcnn import losses as losses\n",
    "import keras_frcnn.roi_helpers as roi_helpers\n",
    "from keras.utils import generic_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras_frcnn import vgg as nn\n",
    "from keras_frcnn.pascal_voc_parser import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(callback, names, logs, batch_no):\n",
    "    for name, value in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(40000) #재귀함수 한도 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./dataset/VOCdevkit\"\n",
    "config_output_filename =\"./config_file/config.pickle\"\n",
    "\n",
    "num_epochs=10\n",
    "#num_epochs=100\n",
    "\n",
    "C = config.Config()\n",
    "C.use_horizontal_flips = False\n",
    "C.use_vertical_flips = False\n",
    "C.rot_90 = False\n",
    "C.model_path = './config_file/model_frcnn.hdf5'\n",
    "C.num_rois = 32 #물체가 있을 거 같은것을 막 뽑아낸다. 한 이미지에서 32개만 뽑음.\n",
    "C.network = 'vgg' # 백본 네트워크를 vgg로 \n",
    "C.base_net_weights = \"./config_file/\"+nn.get_weight_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOC 2012 : http://host.robots.ox.ac.uk/pascal/VOC/voc2012/\n",
    "\n",
    "총 20가지 class. Classification, Object detection, Segementaion을 성능 측정을 위한 정보 제공\n",
    "\n",
    "- 사람 : 사람\n",
    "\n",
    "- 동물 : 새, 고양이, 소, 개, 말, 양\n",
    "\n",
    "- 탈것 : 비행기, 자전거, 보트, 버스, 승용차, 오토바이, 기차\n",
    "\n",
    "- 사물 : 병, 의자, 식탁, 화초, 소파, 모니터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-27 12:43:06--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1999639040 (1.9G) [application/x-tar]\n",
      "Saving to: ‘./dataset/VOCtrainval_11-May-2012.tar’\n",
      "\n",
      "2.tar                54%[=========>          ]   1.02G   111KB/s    eta 52m 48s"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(train_path):\n",
    "    !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar -P ./dataset\n",
    "    !tar -xvf ./dataset/VOCtrainval_11-May-2012.tar -C ./dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xml 파일 parsing\n",
    "all_imgs, classes_count, class_mapping = get_data(train_path)\n",
    "\n",
    "# background 클래스 추가 20개 클래스에 bg까지 21개\n",
    "if 'bg' not in classes_count:\n",
    "    classes_count['bg'] = 0\n",
    "    class_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "C.class_mapping = class_mapping\n",
    "\n",
    "inv_map = {v: k for k, v in class_mapping.items()}\n",
    "print('Training images per class:')\n",
    "pprint.pprint(classes_count)\n",
    "print('Num classes (including bg) = {}'.format(len(classes_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정값 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_output_filename, 'wb') as config_f:\n",
    "    pickle.dump(C, config_f)\n",
    "    print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_imgs)\n",
    "num_imgs = len(all_imgs)\n",
    "\n",
    "train_imgs = [s for s in all_imgs if s['imageset'] == 'train']\n",
    "val_imgs = [s for s in all_imgs if s['imageset'] == 'val']\n",
    "test_imgs = [s for s in all_imgs if s['imageset'] == 'test']\n",
    "\n",
    "print('Num train samples {}'.format(len(train_imgs)))\n",
    "print('Num val samples {}'.format(len(val_imgs)))\n",
    "print('Num test samples {}'.format(len(test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groundtruth anchor 데이터 가져오기 Region Proposal Net 학습하기 위한 codes\n",
    "data_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train')\n",
    "data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='val')\n",
    "data_gen_test = data_generators.get_anchor_gt(test_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='val')\n",
    "\n",
    "input_shape_img = (None, None, 3)\n",
    "\n",
    "# input placeholder 정의\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base network(feature extractor) 정의\n",
    "from keras_frcnn import vgg as nn_base\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# define the RPN, built on the base layers\n",
    "# RPN 정의\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "# detection network 정의\n",
    "classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "\n",
    "if not os.path.exists(C.base_net_weights):\n",
    "    !wget https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5  -P ./config_file\n",
    "\n",
    "print('loading weights from {}'.format(C.base_net_weights))\n",
    "model_rpn.load_weights(C.base_net_weights, by_name=True)\n",
    "model_classifier.load_weights(C.base_net_weights, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=1e-5)\n",
    "optimizer_classifier = Adam(lr=1e-5)\n",
    "model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\n",
    "model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "model_all.compile(optimizer='sgd', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(model_all, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard log폴더 생성\n",
    "log_path = './logs'\n",
    "if not os.path.isdir(log_path):\n",
    "    os.mkdir(log_path)\n",
    "\n",
    "# Tensorboard log모델 연결\n",
    "callback = TensorBoard(log_path)\n",
    "callback.set_model(model_all)\n",
    "\n",
    "epoch_length = 1000\n",
    "num_epochs = int(num_epochs)\n",
    "iter_num = 0\n",
    "train_step = 0\n",
    "\n",
    "losses = np.zeros((epoch_length, 5))\n",
    "rpn_accuracy_rpn_monitor = []\n",
    "rpn_accuracy_for_epoch = []\n",
    "start_time = time.time()\n",
    "\n",
    "best_loss = np.Inf\n",
    "\n",
    "class_mapping_inv = {v: k for k, v in class_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_num in range(num_epochs):\n",
    "\n",
    "    progbar = generic_utils.Progbar(epoch_length)   # keras progress bar 사용\n",
    "    print('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n",
    "\n",
    "    while True:\n",
    "        # try:\n",
    "        # mean overlapping bboxes 출력\n",
    "        if len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n",
    "            mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
    "            rpn_accuracy_rpn_monitor = []\n",
    "            print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n",
    "            if mean_overlapping_bboxes == 0:\n",
    "                print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
    "\n",
    "        # data generator에서 X, Y, image 가져오기\n",
    "        X, Y, img_data = next(data_gen_train)\n",
    "\n",
    "        loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "        write_log(callback, ['rpn_cls_loss', 'rpn_reg_loss'], loss_rpn, train_step)\n",
    "\n",
    "        P_rpn = model_rpn.predict_on_batch(X)\n",
    "\n",
    "        R = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
    "        # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "        X2, Y1, Y2, IouS = roi_helpers.calc_iou(R, img_data, C, class_mapping)\n",
    "\n",
    "        if X2 is None:\n",
    "            rpn_accuracy_rpn_monitor.append(0)\n",
    "            rpn_accuracy_for_epoch.append(0)\n",
    "            continue\n",
    "\n",
    "        # sampling positive/negative samples\n",
    "        neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "        pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "        if len(neg_samples) > 0:\n",
    "            neg_samples = neg_samples[0]\n",
    "        else:\n",
    "            neg_samples = []\n",
    "\n",
    "        if len(pos_samples) > 0:\n",
    "            pos_samples = pos_samples[0]\n",
    "        else:\n",
    "            pos_samples = []\n",
    "\n",
    "        rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "        rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "\n",
    "        if C.num_rois > 1:\n",
    "            if len(pos_samples) < C.num_rois//2:\n",
    "                selected_pos_samples = pos_samples.tolist()\n",
    "            else:\n",
    "                selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n",
    "            try:\n",
    "                selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "            except:\n",
    "                selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "\n",
    "            sel_samples = selected_pos_samples + selected_neg_samples\n",
    "        else:\n",
    "            # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "            selected_pos_samples = pos_samples.tolist()\n",
    "            selected_neg_samples = neg_samples.tolist()\n",
    "            if np.random.randint(0, 2):\n",
    "                sel_samples = random.choice(neg_samples)\n",
    "            else:\n",
    "                sel_samples = random.choice(pos_samples)\n",
    "\n",
    "        loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "        write_log(callback, ['detection_cls_loss', 'detection_reg_loss', 'detection_acc'], loss_class, train_step)\n",
    "        train_step += 1\n",
    "\n",
    "        losses[iter_num, 0] = loss_rpn[1]\n",
    "        losses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "        losses[iter_num, 2] = loss_class[1]\n",
    "        losses[iter_num, 3] = loss_class[2]\n",
    "        losses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "        iter_num += 1\n",
    "\n",
    "        progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n",
    "                                  ('detector_cls', np.mean(losses[:iter_num, 2])), ('detector_regr', np.mean(losses[:iter_num, 3]))])\n",
    "\n",
    "        if iter_num == epoch_length:\n",
    "            loss_rpn_cls = np.mean(losses[:, 0])\n",
    "            loss_rpn_regr = np.mean(losses[:, 1])\n",
    "            loss_class_cls = np.mean(losses[:, 2])\n",
    "            loss_class_regr = np.mean(losses[:, 3])\n",
    "            class_acc = np.mean(losses[:, 4])\n",
    "\n",
    "            mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "            rpn_accuracy_for_epoch = []\n",
    "\n",
    "            if C.verbose:\n",
    "                print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "                print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "                print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "                print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "                print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "                print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "                print('Elapsed time: {}'.format(time.time() - start_time))\n",
    "\n",
    "            curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "            iter_num = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            write_log(callback,\n",
    "                      ['Elapsed_time', 'mean_overlapping_bboxes', 'mean_rpn_cls_loss', 'mean_rpn_reg_loss',\n",
    "                       'mean_detection_cls_loss', 'mean_detection_reg_loss', 'mean_detection_acc', 'total_loss'],\n",
    "                      [time.time() - start_time, mean_overlapping_bboxes, loss_rpn_cls, loss_rpn_regr,\n",
    "                       loss_class_cls, loss_class_regr, class_acc, curr_loss],\n",
    "                      epoch_num)\n",
    "\n",
    "            if curr_loss < best_loss:\n",
    "                if C.verbose:\n",
    "                    print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
    "                best_loss = curr_loss\n",
    "                model_all.save_weights(C.model_path)\n",
    "\n",
    "            break\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print('Exception: {}'.format(e))\n",
    "        #     continue\n",
    "\n",
    "print('Training complete, exiting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "343px",
    "left": "410.667px",
    "right": "20px",
    "top": "24px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
