{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.datasets.cifar100 import load_data\n",
    "import tensorflow.contrib as tf_contrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization\n",
    "def normalize(X_train, X_test):\n",
    "    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    return X_train, X_test\n",
    "x_train, x_test = normalize(x_train, x_test)\n",
    "\n",
    "# Data를 Shuffle해줌\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer 함수\n",
    "def conv(x, channels, kernel=4, stride=2, padding='SAME', use_bias=True, scope='conv_0'):\n",
    "    with tf.variable_scope(scope):\n",
    "        x = tf.layers.conv2d(inputs=x, filters=channels,\n",
    "                             kernel_size=kernel, kernel_initializer=weight_init,\n",
    "                             strides=stride, use_bias=use_bias, padding=padding)\n",
    "    return x\n",
    "# ResNet block Layers 함수 구현\n",
    "def resblock(x_init, channels, is_training=True, use_bias=True, downsample=False, scope='resblock') :\n",
    "    with tf.variable_scope(scope) :\n",
    "        x = batch_norm(x_init, is_training, scope='batch_norm_0')\n",
    "        x = tf.nn.relu(x)\n",
    "        if downsample :\n",
    "            x = conv(x, channels, kernel=3, stride=2, use_bias=use_bias, scope='conv_0')\n",
    "            x_init = conv(x_init, channels, kernel=1, stride=2,\n",
    "                          use_bias=use_bias, scope='conv_init')\n",
    "        else :\n",
    "            x = conv(x, channels, kernel=3, stride=1,\n",
    "                     use_bias=use_bias, scope='conv_0')\n",
    "        x = batch_norm(x, is_training, scope='batch_norm_1')\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_1')\n",
    "\n",
    "        return x + x_init\n",
    "\n",
    "    return x\n",
    "# Batch Normalization 함수 구현\n",
    "def batch_norm(x, is_training=True, scope='batch_norm'):\n",
    "    return tf_contrib.layers.batch_norm(x,\n",
    "                                        decay=0.9, epsilon=1e-05,\n",
    "                                        center=True, scale=True, updates_collections=None,\n",
    "                                        is_training=is_training, scope=scope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters Settings\n",
    "learning_rate = 0.0015\n",
    "num_epochs = 100\n",
    "batch_size = 200\n",
    "num_display = 200\n",
    "weight_init = tf_contrib.layers.xavier_initializer_conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Networks 구현\n",
    "\n",
    "def get_model(X, by, is_training=True, reuse=False):\n",
    "    with tf.variable_scope(\"network\", reuse=reuse):\n",
    "        residual_list = [2, 2, 2, 3]\n",
    "#         residual_list = [3,4,6,3]\n",
    "        ch = 32 # paper is 64\n",
    "        x = conv(X, channels=ch, kernel=3, stride=1, scope='conv')\n",
    "\n",
    "        for i in range(residual_list[0]) :\n",
    "            x = resblock(x, channels=ch, is_training=is_training,\n",
    "                         downsample=False, scope='resblock0_' + str(i))\n",
    "\n",
    "        x = resblock(x, channels=ch*2, is_training=is_training,\n",
    "                     downsample=True, scope='resblock1_0')\n",
    "\n",
    "        for i in range(1, residual_list[1]) :\n",
    "            x = resblock(x, channels=ch*2, is_training=is_training,\n",
    "                         downsample=False, scope='resblock1_' + str(i))\n",
    "\n",
    "        x = resblock(x, channels=ch*4, is_training=is_training,\n",
    "                     downsample=True, scope='resblock2_0')\n",
    "\n",
    "        for i in range(1, residual_list[2]) :\n",
    "            x = resblock(x, channels=ch*4, is_training=is_training, downsample=False, scope='resblock2_' + str(i))\n",
    "\n",
    "        x = tf.layers.dropout(x, 0.7 ,training=training)\n",
    "        x = resblock(x, channels=ch*8, is_training=is_training, downsample=True, scope='resblock_3_0')\n",
    "\n",
    "        for i in range(1, residual_list[3]) :\n",
    "            x = resblock(x, channels=ch*8, is_training=is_training, downsample=False, scope='resblock_3_' + str(i))\n",
    "        x = batch_norm(x, is_training, scope='batch_norm')\n",
    "        x = tf.nn.relu(x)\n",
    "        x=tf.layers.average_pooling2d(x, pool_size=2, strides=2, padding='SAME')\n",
    "        flat = tf.reshape(x,\n",
    "                          (-1, x.shape[1]*x.shape[2]*x.shape[3]))\n",
    "        with tf.variable_scope('dense'):\n",
    "            dense1 = tf.layers.dense(flat, 386,\n",
    "                                  kernel_initializer=weight_init)\n",
    "            dense_h = tf.nn.leaky_relu(dense1)\n",
    "            dense_dropout = tf.layers.dropout(dense_h,0.5)\n",
    "#             dense2 = tf.layers.dense(dense_dropout, 128,\n",
    "#                                   kernel_initializer=weight_init)\n",
    "#             dense_h = tf.nn.relu(dense2)\n",
    "#             dense3 = tf.layers.dropout(dense_h,0.3)\n",
    "            outputs = tf.layers.dense(dense_dropout, 100)\n",
    "        one_hot = tf.squeeze(tf.one_hot(by, 100),axis=1) #이 부분을 tf.squeeze로 바꾸고 난뒤 학습이 되기 시작함.\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, \n",
    "                                                          labels=one_hot))\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate,).minimize(loss)\n",
    "        softmax = tf.nn.softmax(outputs)\n",
    "        preds = tf.argmax(softmax, axis=1)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(by, axis=1), preds), tf.float32))\n",
    "        init = tf.global_variables_initializer()\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'opt': opt,\n",
    "        'preds': preds,\n",
    "        'acc': acc,\n",
    "        'init': init,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3],name = 'input_data')\n",
    "by = tf.placeholder(tf.int64, name ='y_input_data')\n",
    "training = tf.placeholder(tf.bool, name = 'training_bool')\n",
    "model = get_model(X, by, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration 1\n",
      "loss 4.6946 acc 0.0250\n",
      "loss 3.7190 acc 0.1450\n",
      "Current iteration 2\n",
      "loss 3.5814 acc 0.1250\n",
      "loss 3.1687 acc 0.2200\n",
      "Current iteration 3\n",
      "loss 3.1155 acc 0.2300\n",
      "loss 2.8086 acc 0.3250\n",
      "Current iteration 4\n",
      "loss 2.8098 acc 0.3100\n",
      "loss 2.4752 acc 0.3750\n",
      "Current iteration 5\n",
      "loss 2.3956 acc 0.3800\n",
      "loss 2.2116 acc 0.4100\n",
      "Current iteration 6\n",
      "loss 2.1498 acc 0.4650\n",
      "loss 1.8996 acc 0.4550\n",
      "Current iteration 7\n",
      "loss 1.9013 acc 0.4850\n",
      "loss 1.6995 acc 0.5100\n",
      "Current iteration 8\n",
      "loss 1.7430 acc 0.5350\n",
      "loss 1.5819 acc 0.5450\n",
      "Current iteration 9\n",
      "loss 1.6165 acc 0.5700\n",
      "loss 1.3545 acc 0.6150\n",
      "Current iteration 10\n",
      "loss 1.4190 acc 0.6100\n",
      "loss 1.2656 acc 0.6550\n",
      "Current iteration 11\n",
      "loss 1.2408 acc 0.6700\n",
      "loss 1.1914 acc 0.6400\n",
      "Current iteration 12\n",
      "loss 1.1833 acc 0.6400\n",
      "loss 0.9459 acc 0.7400\n",
      "Current iteration 13\n",
      "loss 0.9519 acc 0.7150\n",
      "loss 0.7825 acc 0.7700\n",
      "Current iteration 14\n",
      "loss 0.8594 acc 0.7450\n",
      "loss 0.7550 acc 0.7800\n",
      "Current iteration 15\n",
      "loss 0.6955 acc 0.8200\n",
      "loss 0.7995 acc 0.7650\n",
      "Current iteration 16\n",
      "loss 0.6186 acc 0.8400\n",
      "loss 0.5848 acc 0.8300\n",
      "Current iteration 17\n",
      "loss 0.7213 acc 0.7950\n",
      "loss 0.5378 acc 0.8300\n",
      "Current iteration 18\n",
      "loss 0.5302 acc 0.8350\n",
      "loss 0.4120 acc 0.8800\n",
      "Current iteration 19\n",
      "loss 0.5126 acc 0.8350\n",
      "loss 0.4040 acc 0.8650\n",
      "Current iteration 20\n",
      "loss 0.4524 acc 0.8400\n",
      "loss 0.3414 acc 0.8850\n",
      "Current iteration 21\n",
      "loss 0.3571 acc 0.8650\n",
      "loss 0.3582 acc 0.9050\n",
      "Current iteration 22\n",
      "loss 0.4152 acc 0.8750\n",
      "loss 0.2926 acc 0.9150\n",
      "Current iteration 23\n",
      "loss 0.3643 acc 0.9050\n",
      "loss 0.2563 acc 0.9150\n",
      "Current iteration 24\n",
      "loss 0.2250 acc 0.9350\n",
      "loss 0.2355 acc 0.9100\n",
      "Current iteration 25\n",
      "loss 0.2922 acc 0.9150\n",
      "loss 0.2473 acc 0.9200\n",
      "Current iteration 26\n",
      "loss 0.2266 acc 0.9100\n",
      "loss 0.2263 acc 0.9200\n",
      "Current iteration 27\n",
      "loss 0.1536 acc 0.9500\n",
      "loss 0.1776 acc 0.9400\n",
      "Current iteration 28\n",
      "loss 0.2234 acc 0.9250\n",
      "loss 0.1297 acc 0.9650\n",
      "Current iteration 29\n",
      "loss 0.1930 acc 0.9200\n",
      "loss 0.1639 acc 0.9300\n",
      "Current iteration 30\n",
      "loss 0.1877 acc 0.9500\n",
      "loss 0.1439 acc 0.9450\n",
      "Current iteration 31\n",
      "loss 0.2149 acc 0.9300\n",
      "loss 0.1562 acc 0.9300\n",
      "Current iteration 32\n",
      "loss 0.1533 acc 0.9400\n",
      "loss 0.1678 acc 0.9350\n",
      "Current iteration 33\n",
      "loss 0.1370 acc 0.9650\n",
      "loss 0.1318 acc 0.9450\n",
      "Current iteration 34\n",
      "loss 0.1910 acc 0.9350\n",
      "loss 0.2056 acc 0.9450\n",
      "Current iteration 35\n",
      "loss 0.1294 acc 0.9550\n",
      "loss 0.1177 acc 0.9700\n",
      "Current iteration 36\n",
      "loss 0.1163 acc 0.9650\n",
      "loss 0.0961 acc 0.9700\n",
      "Current iteration 37\n",
      "loss 0.1173 acc 0.9650\n",
      "loss 0.1360 acc 0.9650\n",
      "Current iteration 38\n",
      "loss 0.1956 acc 0.9450\n",
      "loss 0.0912 acc 0.9750\n",
      "Current iteration 39\n",
      "loss 0.0807 acc 0.9850\n",
      "loss 0.1376 acc 0.9450\n",
      "Current iteration 40\n",
      "loss 0.1058 acc 0.9700\n",
      "loss 0.1539 acc 0.9500\n",
      "Current iteration 41\n",
      "loss 0.1010 acc 0.9650\n",
      "loss 0.0593 acc 0.9800\n",
      "Current iteration 42\n",
      "loss 0.1123 acc 0.9700\n",
      "loss 0.1744 acc 0.9300\n",
      "Current iteration 43\n",
      "loss 0.0858 acc 0.9900\n",
      "loss 0.1948 acc 0.9300\n",
      "Current iteration 44\n",
      "loss 0.1674 acc 0.9300\n",
      "loss 0.0741 acc 0.9850\n",
      "Current iteration 45\n",
      "loss 0.1471 acc 0.9550\n",
      "loss 0.1332 acc 0.9700\n",
      "Current iteration 46\n",
      "loss 0.1365 acc 0.9550\n",
      "loss 0.0698 acc 0.9850\n",
      "Current iteration 47\n",
      "loss 0.0687 acc 0.9700\n",
      "loss 0.0775 acc 0.9800\n",
      "Current iteration 48\n",
      "loss 0.1179 acc 0.9550\n",
      "loss 0.1138 acc 0.9350\n",
      "Current iteration 49\n",
      "loss 0.0982 acc 0.9750\n",
      "loss 0.0672 acc 0.9850\n",
      "Current iteration 50\n",
      "loss 0.1161 acc 0.9650\n",
      "loss 0.1698 acc 0.9450\n",
      "Current iteration 51\n",
      "loss 0.0556 acc 0.9850\n",
      "loss 0.0933 acc 0.9700\n",
      "Current iteration 52\n",
      "loss 0.0938 acc 0.9750\n",
      "loss 0.0625 acc 0.9800\n",
      "Current iteration 53\n",
      "loss 0.0611 acc 0.9750\n",
      "loss 0.1169 acc 0.9550\n",
      "Current iteration 54\n",
      "loss 0.0380 acc 0.9950\n",
      "loss 0.1384 acc 0.9600\n",
      "Current iteration 55\n",
      "loss 0.1051 acc 0.9650\n",
      "loss 0.1128 acc 0.9550\n",
      "Current iteration 56\n",
      "loss 0.0616 acc 0.9800\n",
      "loss 0.1044 acc 0.9700\n",
      "Current iteration 57\n",
      "loss 0.1247 acc 0.9600\n",
      "loss 0.0777 acc 0.9650\n",
      "Current iteration 58\n",
      "loss 0.1826 acc 0.9550\n",
      "loss 0.0840 acc 0.9700\n",
      "Current iteration 59\n",
      "loss 0.1176 acc 0.9600\n",
      "loss 0.0615 acc 0.9750\n",
      "Current iteration 60\n",
      "loss 0.1217 acc 0.9700\n",
      "loss 0.0711 acc 0.9700\n",
      "Current iteration 61\n",
      "loss 0.1240 acc 0.9550\n",
      "loss 0.0762 acc 0.9700\n",
      "Current iteration 62\n",
      "loss 0.1575 acc 0.9550\n",
      "loss 0.0627 acc 0.9800\n",
      "Current iteration 63\n",
      "loss 0.0826 acc 0.9800\n",
      "loss 0.1181 acc 0.9650\n",
      "Current iteration 64\n",
      "loss 0.1245 acc 0.9600\n",
      "loss 0.0734 acc 0.9750\n",
      "Current iteration 65\n",
      "loss 0.0623 acc 0.9800\n",
      "loss 0.0995 acc 0.9650\n",
      "Current iteration 66\n",
      "loss 0.1244 acc 0.9700\n",
      "loss 0.0644 acc 0.9800\n",
      "Current iteration 67\n",
      "loss 0.0953 acc 0.9750\n",
      "loss 0.0909 acc 0.9650\n",
      "Current iteration 68\n",
      "loss 0.0641 acc 0.9850\n",
      "loss 0.0660 acc 0.9700\n",
      "Current iteration 69\n",
      "loss 0.0781 acc 0.9700\n",
      "loss 0.0845 acc 0.9750\n",
      "Current iteration 70\n",
      "loss 0.0896 acc 0.9700\n",
      "loss 0.0863 acc 0.9650\n",
      "Current iteration 71\n",
      "loss 0.0946 acc 0.9700\n",
      "loss 0.1113 acc 0.9750\n",
      "Current iteration 72\n",
      "loss 0.0441 acc 0.9800\n",
      "loss 0.1218 acc 0.9500\n",
      "Current iteration 73\n",
      "loss 0.0420 acc 0.9800\n",
      "loss 0.0629 acc 0.9850\n",
      "Current iteration 74\n",
      "loss 0.0650 acc 0.9700\n",
      "loss 0.0462 acc 0.9800\n",
      "Current iteration 75\n",
      "loss 0.0683 acc 0.9750\n",
      "loss 0.0638 acc 0.9750\n",
      "Current iteration 76\n",
      "loss 0.0897 acc 0.9650\n",
      "loss 0.0465 acc 0.9900\n",
      "Current iteration 77\n",
      "loss 0.0573 acc 0.9800\n",
      "loss 0.0866 acc 0.9700\n",
      "Current iteration 78\n",
      "loss 0.0345 acc 0.9900\n",
      "loss 0.0749 acc 0.9750\n",
      "Current iteration 79\n",
      "loss 0.0871 acc 0.9750\n",
      "loss 0.0564 acc 0.9750\n",
      "Current iteration 80\n",
      "loss 0.1007 acc 0.9700\n",
      "loss 0.1126 acc 0.9650\n",
      "Current iteration 81\n",
      "loss 0.1018 acc 0.9700\n",
      "loss 0.0310 acc 0.9900\n",
      "Current iteration 82\n",
      "loss 0.1241 acc 0.9650\n",
      "loss 0.0310 acc 0.9900\n",
      "Current iteration 83\n",
      "loss 0.1236 acc 0.9750\n",
      "loss 0.0526 acc 0.9800\n",
      "Current iteration 84\n",
      "loss 0.0708 acc 0.9750\n",
      "loss 0.1140 acc 0.9700\n",
      "Current iteration 85\n",
      "loss 0.0262 acc 0.9900\n",
      "loss 0.0148 acc 0.9950\n",
      "Current iteration 86\n",
      "loss 0.0684 acc 0.9800\n",
      "loss 0.0311 acc 0.9850\n",
      "Current iteration 87\n",
      "loss 0.0285 acc 0.9900\n",
      "loss 0.1179 acc 0.9600\n",
      "Current iteration 88\n",
      "loss 0.0186 acc 1.0000\n",
      "loss 0.1038 acc 0.9700\n",
      "Current iteration 89\n",
      "loss 0.0648 acc 0.9800\n",
      "loss 0.0861 acc 0.9700\n",
      "Current iteration 90\n",
      "loss 0.0150 acc 0.9950\n",
      "loss 0.0465 acc 0.9800\n",
      "Current iteration 91\n",
      "loss 0.0179 acc 0.9950\n",
      "loss 0.1454 acc 0.9450\n",
      "Current iteration 92\n",
      "loss 0.0842 acc 0.9700\n",
      "loss 0.0220 acc 1.0000\n",
      "Current iteration 93\n",
      "loss 0.0992 acc 0.9650\n",
      "loss 0.0244 acc 0.9900\n",
      "Current iteration 94\n",
      "loss 0.0753 acc 0.9750\n",
      "loss 0.0598 acc 0.9800\n",
      "Current iteration 95\n",
      "loss 0.0495 acc 0.9850\n",
      "loss 0.1141 acc 0.9650\n",
      "Current iteration 96\n",
      "loss 0.0414 acc 0.9900\n",
      "loss 0.0262 acc 0.9900\n",
      "Current iteration 97\n",
      "loss 0.0783 acc 0.9800\n",
      "loss 0.0807 acc 0.9650\n",
      "Current iteration 98\n",
      "loss 0.0560 acc 0.9850\n",
      "loss 0.0609 acc 0.9800\n",
      "Current iteration 99\n",
      "loss 0.0288 acc 0.9800\n",
      "loss 0.0602 acc 0.9750\n",
      "Current iteration 100\n",
      "loss 0.0342 acc 0.9950\n",
      "loss 0.0680 acc 0.9700\n",
      "TEST: loss 3.7350 acc 0.5533\n",
      "LR 0.0015\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(model['init'])\n",
    "    for ind_epoch in range(0, num_epochs):\n",
    "        print('Current iteration {}'.format(ind_epoch + 1))\n",
    "        \n",
    "        for ind_ in range(0, int(50000 / batch_size)):\n",
    "            batch_X = x_train[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            batch_by = y_train[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            _, cur_loss, cur_acc = sess.run(\n",
    "                [model['opt'], model['loss'], model['acc']],\n",
    "                feed_dict={X: batch_X, by: batch_by, training :True})\n",
    "#             print(sess.run(model['preds'], feed_dict={X:x_train[:10], training:False}))\n",
    "            if ind_ % num_display == 0:\n",
    "                print('loss {0:.4f} acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "    cur_acc_all = 0.0\n",
    "    cur_loss_all = 0.0\n",
    "    for ind_ in range(0, 10):\n",
    "        cur_loss, cur_acc = sess.run(\n",
    "                    [model['loss'], model['acc']],\n",
    "                    feed_dict={X: x_test[ind_*1000:(ind_+1)*1000], \n",
    "                               by: y_test[ind_*1000:(ind_+1)*1000],\n",
    "                              training : False})\n",
    "        cur_loss_all += cur_loss\n",
    "        cur_acc_all += cur_acc\n",
    "    print('TEST: loss {0:.4f} acc {1:.4f}'.format(cur_loss_all / 10.0, \n",
    "                                              cur_acc_all / 10.0))\n",
    "    print('LR',learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current iteration 1\n",
    "loss 4.6968 acc 0.0067\n",
    "loss 4.3137 acc 0.0400\n",
    "Current iteration 2\n",
    "loss 4.2011 acc 0.0400\n",
    "loss 3.7758 acc 0.1067\n",
    "TEST: loss 3.6424 acc 0.1341\n",
    "LR 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Current iteration 1   \n",
    "\n",
    "loss 4.6909 acc 0.0133\n",
    "loss 3.8374 acc 0.1067\n",
    "Current iteration 2\n",
    "loss 3.8082 acc 0.1067\n",
    "loss 3.4199 acc 0.1533\n",
    "TEST: loss 3.1722 acc 0.2228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
