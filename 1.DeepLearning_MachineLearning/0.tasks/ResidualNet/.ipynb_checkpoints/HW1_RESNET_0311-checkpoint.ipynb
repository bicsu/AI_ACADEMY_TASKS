{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from augment import augment\n",
    "import os\n",
    "from keras.datasets.cifar100 import load_data\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow.contrib as tf_contrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization\n",
    "def normalize(X_train, X_test):\n",
    "    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    return X_train, X_test\n",
    "x_train, x_test = normalize(x_train, x_test)\n",
    "\n",
    "# Data를 Shuffle해줌\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer 함수\n",
    "def conv(x, channels, kernel=4, stride=2, padding='SAME', use_bias=True, scope='conv_0'):\n",
    "    with tf.variable_scope(scope):\n",
    "        x = tf.layers.conv2d(inputs=x, filters=channels,\n",
    "                             kernel_size=kernel, kernel_initializer=weight_init,\n",
    "                             kernel_regularizer=weight_regularizer,\n",
    "                             strides=stride, use_bias=use_bias, padding=padding)\n",
    "    return x\n",
    "# ResNet block Layers 함수 구현\n",
    "def resblock(x_init, channels, is_training=True, use_bias=True, downsample=False, scope='resblock') :\n",
    "    with tf.variable_scope(scope) :\n",
    "\n",
    "        x = batch_norm(x_init, is_training, scope='batch_norm_0')\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "\n",
    "        if downsample :\n",
    "            x = conv(x, channels, kernel=3, stride=2, use_bias=use_bias, scope='conv_0')\n",
    "            x_init = conv(x_init, channels, kernel=1, stride=2,\n",
    "                          use_bias=use_bias, scope='conv_init')\n",
    "\n",
    "        else :\n",
    "            x = conv(x, channels, kernel=3, stride=1,\n",
    "                     use_bias=use_bias, scope='conv_0')\n",
    "\n",
    "        x = batch_norm(x, is_training, scope='batch_norm_1')\n",
    "        x = tf.nn.relu(x)\n",
    "        x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_1')\n",
    "\n",
    "        return x + x_init\n",
    "\n",
    "    return x\n",
    "# Batch Normalization 함수 구현\n",
    "def batch_norm(x, is_training=True, scope='batch_norm'):\n",
    "    return tf_contrib.layers.batch_norm(x,\n",
    "                                        decay=0.9, epsilon=1e-05,\n",
    "                                        center=True, scale=True, updates_collections=None,\n",
    "                                        is_training=is_training, scope=scope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters Settings\n",
    "learning_rate = 0.00015\n",
    "num_epochs = 150\n",
    "batch_size = 150\n",
    "num_display = 100\n",
    "weight_init = tf_contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Networks 구현\n",
    "\n",
    "def get_model(X, by, is_training=True, reuse=False):\n",
    "    with tf.variable_scope(\"network\", reuse=reuse):\n",
    "        residual_list = [2, 2, 2, 2]\n",
    "        ch = 16 # paper is 64\n",
    "        x = conv(X, channels=ch, kernel=3, stride=1, scope='conv')\n",
    "\n",
    "        for i in range(residual_list[0]) :\n",
    "            x = resblock(x, channels=ch, is_training=is_training,\n",
    "                         downsample=False, scope='resblock0_' + str(i))\n",
    "\n",
    "        x = resblock(x, channels=ch*2, is_training=is_training,\n",
    "                     downsample=True, scope='resblock1_0')\n",
    "\n",
    "        for i in range(1, residual_list[1]) :\n",
    "            x = resblock(x, channels=ch*2, is_training=is_training,\n",
    "                         downsample=False, scope='resblock1_' + str(i))\n",
    "\n",
    "        x = resblock(x, channels=ch*4, is_training=is_training,\n",
    "                     downsample=True, scope='resblock2_0')\n",
    "\n",
    "        for i in range(1, residual_list[2]) :\n",
    "            x = resblock(x, channels=ch*4, is_training=is_training, downsample=False, scope='resblock2_' + str(i))\n",
    "\n",
    "        x = tf.layers.dropout(x, 0.5 ,training=training)\n",
    "        x = resblock(x, channels=ch*8, is_training=is_training, downsample=True, scope='resblock_3_0')\n",
    "\n",
    "        for i in range(1, residual_list[3]) :\n",
    "            x = resblock(x, channels=ch*8, is_training=is_training, downsample=False, scope='resblock_3_' + str(i))\n",
    "        x = batch_norm(x, is_training, scope='batch_norm')\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        flat = tf.reshape(x,\n",
    "                          (-1, x.shape[1]*x.shape[2]*x.shape[3]))\n",
    "        with tf.variable_scope('dense'):\n",
    "            dense1 = tf.layers.dense(flat, 386,\n",
    "                                  kernel_initializer=tf.initializers.truncated_normal(stddev=0.02))\n",
    "            dense_h = tf.nn.relu(dense1)\n",
    "            dense_dropout = tf.layers.dropout(dense_h,0.5)\n",
    "            outputs = tf.layers.dense(dense_h, 100)\n",
    "        one_hot = tf.squeeze(tf.one_hot(by, 100),axis=1) #이 부분을 tf.squeeze로 바꾸고 난뒤 학습이 되기 시작함.\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, \n",
    "                                                          labels=one_hot))\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate,).minimize(loss)\n",
    "        softmax = tf.nn.softmax(outputs)\n",
    "        preds = tf.argmax(softmax, axis=1)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(by, axis=1), preds), tf.float32))\n",
    "        init = tf.global_variables_initializer()\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'opt': opt,\n",
    "        'preds': preds,\n",
    "        'acc': acc,\n",
    "        'init': init,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3],name = 'input_data')\n",
    "by = tf.placeholder(tf.int64, name ='y_input_data')\n",
    "training = tf.placeholder(tf.bool, name = 'training_bool')\n",
    "model = get_model(X, by, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration 1\n",
      "loss 3.9390 acc 0.1267\n",
      "Current iteration 2\n",
      "loss 3.5863 acc 0.1667\n",
      "Current iteration 3\n",
      "loss 3.3128 acc 0.2733\n",
      "Current iteration 4\n",
      "loss 3.1380 acc 0.2867\n",
      "Current iteration 5\n",
      "loss 2.8974 acc 0.3267\n",
      "Current iteration 6\n",
      "loss 2.8389 acc 0.3533\n",
      "Current iteration 7\n",
      "loss 2.6868 acc 0.3267\n",
      "Current iteration 8\n",
      "loss 2.6165 acc 0.3333\n",
      "Current iteration 9\n",
      "loss 2.5038 acc 0.4067\n",
      "Current iteration 10\n",
      "loss 2.4067 acc 0.4400\n",
      "Current iteration 11\n",
      "loss 2.3286 acc 0.4400\n",
      "Current iteration 12\n",
      "loss 2.1749 acc 0.4133\n",
      "Current iteration 13\n",
      "loss 2.1454 acc 0.4467\n",
      "Current iteration 14\n",
      "loss 2.1287 acc 0.4733\n",
      "Current iteration 15\n",
      "loss 2.0276 acc 0.5400\n",
      "Current iteration 16\n",
      "loss 1.9354 acc 0.5267\n",
      "Current iteration 17\n",
      "loss 1.9172 acc 0.5133\n",
      "Current iteration 18\n",
      "loss 1.7898 acc 0.5000\n",
      "Current iteration 19\n",
      "loss 1.8236 acc 0.5400\n",
      "Current iteration 20\n",
      "loss 1.6598 acc 0.6000\n",
      "Current iteration 21\n",
      "loss 1.6006 acc 0.5733\n",
      "Current iteration 22\n",
      "loss 1.5810 acc 0.5333\n",
      "Current iteration 23\n",
      "loss 1.5047 acc 0.5733\n",
      "Current iteration 24\n",
      "loss 1.5212 acc 0.6067\n",
      "Current iteration 25\n",
      "loss 1.2582 acc 0.6333\n",
      "Current iteration 26\n",
      "loss 1.2908 acc 0.6333\n",
      "Current iteration 27\n",
      "loss 1.1790 acc 0.6667\n",
      "Current iteration 28\n",
      "loss 1.1336 acc 0.6667\n",
      "Current iteration 29\n",
      "loss 1.1619 acc 0.6400\n",
      "Current iteration 30\n",
      "loss 1.1012 acc 0.6867\n",
      "Current iteration 31\n",
      "loss 0.9488 acc 0.7133\n",
      "Current iteration 32\n",
      "loss 1.0348 acc 0.6733\n",
      "Current iteration 33\n",
      "loss 0.9709 acc 0.7400\n",
      "Current iteration 34\n",
      "loss 0.8540 acc 0.8000\n",
      "Current iteration 35\n",
      "loss 0.8354 acc 0.7333\n",
      "Current iteration 36\n",
      "loss 0.7730 acc 0.7867\n",
      "Current iteration 37\n",
      "loss 0.7817 acc 0.7600\n",
      "Current iteration 38\n",
      "loss 0.7151 acc 0.8000\n",
      "Current iteration 39\n",
      "loss 0.6557 acc 0.8600\n",
      "Current iteration 40\n",
      "loss 0.6324 acc 0.8400\n",
      "Current iteration 41\n",
      "loss 0.6680 acc 0.7733\n",
      "Current iteration 42\n",
      "loss 0.6529 acc 0.7733\n",
      "Current iteration 43\n",
      "loss 0.4819 acc 0.8600\n",
      "Current iteration 44\n",
      "loss 0.4886 acc 0.8867\n",
      "Current iteration 45\n",
      "loss 0.3718 acc 0.9067\n",
      "Current iteration 46\n",
      "loss 0.4495 acc 0.8667\n",
      "Current iteration 47\n",
      "loss 0.3603 acc 0.9067\n",
      "Current iteration 48\n",
      "loss 0.4720 acc 0.8667\n",
      "Current iteration 49\n",
      "loss 0.3493 acc 0.9000\n",
      "Current iteration 50\n",
      "loss 0.2899 acc 0.9200\n",
      "Current iteration 51\n",
      "loss 0.3287 acc 0.8933\n",
      "Current iteration 52\n",
      "loss 0.3352 acc 0.8933\n",
      "Current iteration 53\n",
      "loss 0.2838 acc 0.9200\n",
      "Current iteration 54\n",
      "loss 0.3210 acc 0.8933\n",
      "Current iteration 55\n",
      "loss 0.2546 acc 0.9133\n",
      "Current iteration 56\n",
      "loss 0.2625 acc 0.9067\n",
      "Current iteration 57\n",
      "loss 0.2563 acc 0.9133\n",
      "Current iteration 58\n",
      "loss 0.2111 acc 0.9400\n",
      "Current iteration 59\n",
      "loss 0.2377 acc 0.9200\n",
      "Current iteration 60\n",
      "loss 0.2503 acc 0.9133\n",
      "Current iteration 61\n",
      "loss 0.2201 acc 0.9400\n",
      "Current iteration 62\n",
      "loss 0.1984 acc 0.9400\n",
      "Current iteration 63\n",
      "loss 0.1736 acc 0.9467\n",
      "Current iteration 64\n",
      "loss 0.1860 acc 0.9267\n",
      "Current iteration 65\n",
      "loss 0.1539 acc 0.9533\n",
      "Current iteration 66\n",
      "loss 0.2090 acc 0.9400\n",
      "Current iteration 67\n",
      "loss 0.1677 acc 0.9533\n",
      "Current iteration 68\n",
      "loss 0.2114 acc 0.9133\n",
      "Current iteration 69\n",
      "loss 0.1303 acc 0.9533\n",
      "Current iteration 70\n",
      "loss 0.1601 acc 0.9400\n",
      "Current iteration 71\n",
      "loss 0.1429 acc 0.9533\n",
      "Current iteration 72\n",
      "loss 0.1441 acc 0.9467\n",
      "Current iteration 73\n",
      "loss 0.2049 acc 0.9267\n",
      "Current iteration 74\n",
      "loss 0.1418 acc 0.9733\n",
      "Current iteration 75\n",
      "loss 0.1322 acc 0.9533\n",
      "Current iteration 76\n",
      "loss 0.1695 acc 0.9667\n",
      "Current iteration 77\n",
      "loss 0.0894 acc 0.9867\n",
      "Current iteration 78\n",
      "loss 0.1210 acc 0.9600\n",
      "Current iteration 79\n",
      "loss 0.1070 acc 0.9800\n",
      "Current iteration 80\n",
      "loss 0.1110 acc 0.9600\n",
      "Current iteration 81\n",
      "loss 0.0879 acc 0.9733\n",
      "Current iteration 82\n",
      "loss 0.1143 acc 0.9600\n",
      "Current iteration 83\n",
      "loss 0.0883 acc 0.9600\n",
      "Current iteration 84\n",
      "loss 0.0884 acc 0.9600\n",
      "Current iteration 85\n",
      "loss 0.1413 acc 0.9467\n",
      "Current iteration 86\n",
      "loss 0.1012 acc 0.9733\n",
      "Current iteration 87\n",
      "loss 0.0998 acc 0.9867\n",
      "Current iteration 88\n",
      "loss 0.1107 acc 0.9667\n",
      "Current iteration 89\n",
      "loss 0.1274 acc 0.9667\n",
      "Current iteration 90\n",
      "loss 0.0916 acc 0.9667\n",
      "Current iteration 91\n",
      "loss 0.1205 acc 0.9533\n",
      "Current iteration 92\n",
      "loss 0.1359 acc 0.9467\n",
      "Current iteration 93\n",
      "loss 0.0536 acc 0.9800\n",
      "Current iteration 94\n",
      "loss 0.1481 acc 0.9400\n",
      "Current iteration 95\n",
      "loss 0.1162 acc 0.9533\n",
      "Current iteration 96\n",
      "loss 0.0790 acc 0.9733\n",
      "Current iteration 97\n",
      "loss 0.0821 acc 0.9733\n",
      "Current iteration 98\n",
      "loss 0.0821 acc 0.9667\n",
      "Current iteration 99\n",
      "loss 0.0561 acc 0.9933\n",
      "Current iteration 100\n",
      "loss 0.0467 acc 0.9867\n",
      "Current iteration 101\n",
      "loss 0.0672 acc 0.9800\n",
      "Current iteration 102\n",
      "loss 0.0549 acc 0.9933\n",
      "Current iteration 103\n",
      "loss 0.0654 acc 0.9867\n",
      "Current iteration 104\n",
      "loss 0.1264 acc 0.9733\n",
      "Current iteration 105\n",
      "loss 0.0536 acc 0.9800\n",
      "Current iteration 106\n",
      "loss 0.0618 acc 0.9867\n",
      "Current iteration 107\n",
      "loss 0.0346 acc 1.0000\n",
      "Current iteration 108\n",
      "loss 0.0971 acc 0.9867\n",
      "Current iteration 109\n",
      "loss 0.0856 acc 0.9733\n",
      "Current iteration 110\n",
      "loss 0.0710 acc 0.9667\n",
      "Current iteration 111\n",
      "loss 0.0479 acc 0.9933\n",
      "Current iteration 112\n",
      "loss 0.0824 acc 0.9867\n",
      "Current iteration 113\n",
      "loss 0.0502 acc 0.9867\n",
      "Current iteration 114\n",
      "loss 0.0682 acc 0.9733\n",
      "Current iteration 115\n",
      "loss 0.0339 acc 0.9867\n",
      "Current iteration 116\n",
      "loss 0.1036 acc 0.9733\n",
      "Current iteration 117\n",
      "loss 0.0552 acc 0.9867\n",
      "Current iteration 118\n",
      "loss 0.0451 acc 0.9933\n",
      "Current iteration 119\n",
      "loss 0.0534 acc 0.9733\n",
      "Current iteration 120\n",
      "loss 0.0687 acc 0.9867\n",
      "Current iteration 121\n",
      "loss 0.0311 acc 0.9933\n",
      "Current iteration 122\n",
      "loss 0.0879 acc 0.9733\n",
      "Current iteration 123\n",
      "loss 0.0386 acc 0.9867\n",
      "Current iteration 124\n",
      "loss 0.0397 acc 0.9867\n",
      "Current iteration 125\n",
      "loss 0.1026 acc 0.9800\n",
      "Current iteration 126\n",
      "loss 0.0946 acc 0.9800\n",
      "Current iteration 127\n",
      "loss 0.0818 acc 0.9733\n",
      "Current iteration 128\n",
      "loss 0.0281 acc 0.9933\n",
      "Current iteration 129\n",
      "loss 0.0657 acc 0.9800\n",
      "Current iteration 130\n",
      "loss 0.0358 acc 0.9867\n",
      "Current iteration 131\n",
      "loss 0.0496 acc 0.9933\n",
      "Current iteration 132\n",
      "loss 0.0835 acc 0.9667\n",
      "Current iteration 133\n",
      "loss 0.0808 acc 0.9800\n",
      "Current iteration 134\n",
      "loss 0.0674 acc 0.9800\n",
      "Current iteration 135\n",
      "loss 0.0560 acc 0.9800\n",
      "Current iteration 136\n",
      "loss 0.1105 acc 0.9733\n",
      "Current iteration 137\n",
      "loss 0.0596 acc 0.9933\n",
      "Current iteration 138\n",
      "loss 0.0310 acc 0.9933\n",
      "Current iteration 139\n",
      "loss 0.0149 acc 1.0000\n",
      "Current iteration 140\n",
      "loss 0.0325 acc 0.9933\n",
      "Current iteration 141\n",
      "loss 0.0427 acc 0.9867\n",
      "Current iteration 142\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(model['init'])\n",
    "    for ind_epoch in range(0, num_epochs):\n",
    "        print('Current iteration {}'.format(ind_epoch + 1))\n",
    "        \n",
    "        for ind_ in range(0, int(50000 / batch_size)):\n",
    "            batch_X = x_train[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            batch_by = y_train[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            _, cur_loss, cur_acc = sess.run(\n",
    "                [model['opt'], model['loss'], model['acc']],\n",
    "                feed_dict={X: batch_X, by: batch_by, training :True})\n",
    "#             print(sess.run(model['preds'], feed_dict={X:x_train[:10], training:False}))\n",
    "#             if ind_ % num_display == 0:\n",
    "        print('loss {0:.4f} acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "    cur_acc_all = 0.0\n",
    "    cur_loss_all = 0.0\n",
    "    for ind_ in range(0, 10):\n",
    "        cur_loss, cur_acc = sess.run(\n",
    "                    [model['loss'], model['acc']],\n",
    "                    feed_dict={X: x_test[ind_*1000:(ind_+1)*1000], \n",
    "                               by: y_test[ind_*1000:(ind_+1)*1000],\n",
    "                              training : False})\n",
    "        cur_loss_all += cur_loss\n",
    "        cur_acc_all += cur_acc\n",
    "    print('TEST: loss {0:.4f} acc {1:.4f}'.format(cur_loss_all / 10.0, \n",
    "                                              cur_acc_all / 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
