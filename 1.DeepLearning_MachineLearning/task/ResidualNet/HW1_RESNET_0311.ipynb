{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# from augment import augment\n",
    "import os\n",
    "from keras.datasets.cifar100 import load_data\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow.contrib as tf_contrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization\n",
    "def normalize(X_train, X_test):\n",
    "    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    return X_train, X_test\n",
    "x_train, x_test = normalize(x_train, x_test)\n",
    "\n",
    "# Data를 Shuffle해줌\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer 함수\n",
    "def conv(x, channels, kernel=4, stride=2, padding='SAME', use_bias=True, scope='conv_0'):\n",
    "    with tf.variable_scope(scope):\n",
    "        x = tf.layers.conv2d(inputs=x, filters=channels,\n",
    "                             kernel_size=kernel, kernel_initializer=weight_init,\n",
    "                             strides=stride, use_bias=use_bias, padding=padding)\n",
    "    return x\n",
    "# ResNet block Layers 함수 구현\n",
    "def resblock(x_init, channels, is_training=True, use_bias=True, downsample=False, scope='resblock') :\n",
    "    with tf.variable_scope(scope) :\n",
    "\n",
    "        x = batch_norm(x_init, is_training, scope='batch_norm_0')\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "\n",
    "        if downsample :\n",
    "            x = conv(x, channels, kernel=3, stride=2, use_bias=use_bias, scope='conv_0')\n",
    "            x_init = conv(x_init, channels, kernel=1, stride=2,\n",
    "                          use_bias=use_bias, scope='conv_init')\n",
    "\n",
    "        else :\n",
    "            x = conv(x, channels, kernel=3, stride=1,\n",
    "                     use_bias=use_bias, scope='conv_0')\n",
    "\n",
    "        x = batch_norm(x, is_training, scope='batch_norm_1')\n",
    "        x = tf.nn.relu(x)\n",
    "        x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_1')\n",
    "\n",
    "        return x + x_init\n",
    "\n",
    "    return x\n",
    "# Batch Normalization 함수 구현\n",
    "def batch_norm(x, is_training=True, scope='batch_norm'):\n",
    "    return tf_contrib.layers.batch_norm(x,\n",
    "                                        decay=0.9, epsilon=1e-05,\n",
    "                                        center=True, scale=True, updates_collections=None,\n",
    "                                        is_training=is_training, scope=scope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters Settings\n",
    "learning_rate = 0.00095\n",
    "num_epochs = 200\n",
    "batch_size = 150\n",
    "num_display = 100\n",
    "weight_init = tf_contrib.layers.xavier_initializer_conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Networks 구현\n",
    "\n",
    "def get_model(X, by, is_training=True, reuse=False):\n",
    "    with tf.variable_scope(\"network\", reuse=reuse):\n",
    "        residual_list = [2, 2, 2, 2]\n",
    "        ch = 16 # paper is 64\n",
    "        x = conv(X, channels=ch, kernel=3, stride=1, scope='conv')\n",
    "\n",
    "        for i in range(residual_list[0]) :\n",
    "            x = resblock(x, channels=ch, is_training=is_training,\n",
    "                         downsample=False, scope='resblock0_' + str(i))\n",
    "\n",
    "        x = resblock(x, channels=ch*2, is_training=is_training,\n",
    "                     downsample=True, scope='resblock1_0')\n",
    "\n",
    "        for i in range(1, residual_list[1]) :\n",
    "            x = resblock(x, channels=ch*2, is_training=is_training,\n",
    "                         downsample=False, scope='resblock1_' + str(i))\n",
    "\n",
    "        x = resblock(x, channels=ch*4, is_training=is_training,\n",
    "                     downsample=True, scope='resblock2_0')\n",
    "\n",
    "        for i in range(1, residual_list[2]) :\n",
    "            x = resblock(x, channels=ch*4, is_training=is_training, downsample=False, scope='resblock2_' + str(i))\n",
    "\n",
    "        x = tf.layers.dropout(x, 0.7 ,training=training)\n",
    "        x = resblock(x, channels=ch*8, is_training=is_training, downsample=True, scope='resblock_3_0')\n",
    "\n",
    "        for i in range(1, residual_list[3]) :\n",
    "            x = resblock(x, channels=ch*8, is_training=is_training, downsample=False, scope='resblock_3_' + str(i))\n",
    "        x = batch_norm(x, is_training, scope='batch_norm')\n",
    "        x = tf.nn.relu(x)\n",
    "        x=tf.layers.average_pooling2d(x, pool_size=2, strides=2, padding='SAME')\n",
    "        flat = tf.reshape(x,\n",
    "                          (-1, x.shape[1]*x.shape[2]*x.shape[3]))\n",
    "        with tf.variable_scope('dense'):\n",
    "            dense1 = tf.layers.dense(flat, 386,\n",
    "                                  kernel_initializer=weight_init)\n",
    "            dense_h = tf.nn.relu(dense1)\n",
    "            dense_dropout = tf.layers.dropout(dense_h,0.5)\n",
    "            outputs = tf.layers.dense(dense_h, 100)\n",
    "        one_hot = tf.squeeze(tf.one_hot(by, 100),axis=1) #이 부분을 tf.squeeze로 바꾸고 난뒤 학습이 되기 시작함.\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, \n",
    "                                                          labels=one_hot))\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate,).minimize(loss)\n",
    "        softmax = tf.nn.softmax(outputs)\n",
    "        preds = tf.argmax(softmax, axis=1)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(by, axis=1), preds), tf.float32))\n",
    "        init = tf.global_variables_initializer()\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'opt': opt,\n",
    "        'preds': preds,\n",
    "        'acc': acc,\n",
    "        'init': init,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3],name = 'input_data')\n",
    "by = tf.placeholder(tf.int64, name ='y_input_data')\n",
    "training = tf.placeholder(tf.bool, name = 'training_bool')\n",
    "model = get_model(X, by, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration 1\n",
      "loss 3.7467 acc 0.1733\n",
      "Current iteration 2\n",
      "loss 3.2744 acc 0.2267\n",
      "Current iteration 3\n",
      "loss 2.9391 acc 0.3000\n",
      "Current iteration 4\n",
      "loss 2.7217 acc 0.3067\n",
      "Current iteration 5\n",
      "loss 2.5173 acc 0.3800\n",
      "Current iteration 6\n",
      "loss 2.3231 acc 0.4200\n",
      "Current iteration 7\n",
      "loss 2.1513 acc 0.4533\n",
      "Current iteration 8\n",
      "loss 2.0874 acc 0.4867\n",
      "Current iteration 9\n",
      "loss 1.9687 acc 0.5467\n",
      "Current iteration 10\n",
      "loss 1.8448 acc 0.5333\n",
      "Current iteration 11\n",
      "loss 1.7655 acc 0.5267\n",
      "Current iteration 12\n",
      "loss 1.7467 acc 0.5267\n",
      "Current iteration 13\n",
      "loss 1.6402 acc 0.5333\n",
      "Current iteration 14\n",
      "loss 1.4573 acc 0.6067\n",
      "Current iteration 15\n",
      "loss 1.4295 acc 0.6200\n",
      "Current iteration 16\n",
      "loss 1.3651 acc 0.6267\n",
      "Current iteration 17\n",
      "loss 1.3133 acc 0.6200\n",
      "Current iteration 18\n",
      "loss 1.2457 acc 0.6267\n",
      "Current iteration 19\n",
      "loss 1.1738 acc 0.6667\n",
      "Current iteration 20\n",
      "loss 1.0808 acc 0.6600\n",
      "Current iteration 21\n",
      "loss 1.0186 acc 0.7267\n",
      "Current iteration 22\n",
      "loss 0.8673 acc 0.7533\n",
      "Current iteration 23\n",
      "loss 0.8591 acc 0.7400\n",
      "Current iteration 24\n",
      "loss 0.8799 acc 0.7267\n",
      "Current iteration 25\n",
      "loss 0.7832 acc 0.7533\n",
      "Current iteration 26\n",
      "loss 0.6056 acc 0.8133\n",
      "Current iteration 27\n",
      "loss 0.6540 acc 0.7867\n",
      "Current iteration 28\n",
      "loss 0.6757 acc 0.7667\n",
      "Current iteration 29\n",
      "loss 0.6096 acc 0.8133\n",
      "Current iteration 30\n",
      "loss 0.6562 acc 0.8067\n",
      "Current iteration 31\n",
      "loss 0.5612 acc 0.8000\n",
      "Current iteration 32\n",
      "loss 0.4216 acc 0.8533\n",
      "Current iteration 33\n",
      "loss 0.4533 acc 0.8533\n",
      "Current iteration 34\n",
      "loss 0.5046 acc 0.8600\n",
      "Current iteration 35\n",
      "loss 0.5886 acc 0.7933\n",
      "Current iteration 36\n",
      "loss 0.3822 acc 0.8933\n",
      "Current iteration 37\n",
      "loss 0.3673 acc 0.8800\n",
      "Current iteration 38\n",
      "loss 0.3769 acc 0.8667\n",
      "Current iteration 39\n",
      "loss 0.3633 acc 0.8733\n",
      "Current iteration 40\n",
      "loss 0.5372 acc 0.8533\n",
      "Current iteration 41\n",
      "loss 0.3918 acc 0.8800\n",
      "Current iteration 42\n",
      "loss 0.3113 acc 0.8800\n",
      "Current iteration 43\n",
      "loss 0.4289 acc 0.8533\n",
      "Current iteration 44\n",
      "loss 0.2937 acc 0.9200\n",
      "Current iteration 45\n",
      "loss 0.2944 acc 0.8867\n",
      "Current iteration 46\n",
      "loss 0.3627 acc 0.9067\n",
      "Current iteration 47\n",
      "loss 0.3883 acc 0.8800\n",
      "Current iteration 48\n",
      "loss 0.3610 acc 0.8800\n",
      "Current iteration 49\n",
      "loss 0.2588 acc 0.9200\n",
      "Current iteration 50\n",
      "loss 0.3896 acc 0.8733\n",
      "Current iteration 51\n",
      "loss 0.3177 acc 0.8800\n",
      "Current iteration 52\n",
      "loss 0.2966 acc 0.9133\n",
      "Current iteration 53\n",
      "loss 0.2488 acc 0.9000\n",
      "Current iteration 54\n",
      "loss 0.2698 acc 0.8933\n",
      "Current iteration 55\n",
      "loss 0.3557 acc 0.9000\n",
      "Current iteration 56\n",
      "loss 0.2711 acc 0.9133\n",
      "Current iteration 57\n",
      "loss 0.4195 acc 0.8467\n",
      "Current iteration 58\n",
      "loss 0.1442 acc 0.9667\n",
      "Current iteration 59\n",
      "loss 0.1466 acc 0.9533\n",
      "Current iteration 60\n",
      "loss 0.2494 acc 0.9267\n",
      "Current iteration 61\n",
      "loss 0.1992 acc 0.9267\n",
      "Current iteration 62\n",
      "loss 0.2705 acc 0.9000\n",
      "Current iteration 63\n",
      "loss 0.2024 acc 0.9333\n",
      "Current iteration 64\n",
      "loss 0.1780 acc 0.9333\n",
      "Current iteration 65\n",
      "loss 0.3278 acc 0.9200\n",
      "Current iteration 66\n",
      "loss 0.0914 acc 0.9800\n",
      "Current iteration 67\n",
      "loss 0.2837 acc 0.8667\n",
      "Current iteration 68\n",
      "loss 0.1440 acc 0.9533\n",
      "Current iteration 69\n",
      "loss 0.1609 acc 0.9600\n",
      "Current iteration 70\n",
      "loss 0.1607 acc 0.9267\n",
      "Current iteration 71\n",
      "loss 0.2656 acc 0.9267\n",
      "Current iteration 72\n",
      "loss 0.2860 acc 0.9067\n",
      "Current iteration 73\n",
      "loss 0.1508 acc 0.9667\n",
      "Current iteration 74\n",
      "loss 0.1614 acc 0.9533\n",
      "Current iteration 75\n",
      "loss 0.1807 acc 0.9467\n",
      "Current iteration 76\n",
      "loss 0.1073 acc 0.9800\n",
      "Current iteration 77\n",
      "loss 0.3257 acc 0.9000\n",
      "Current iteration 78\n",
      "loss 0.1043 acc 0.9600\n",
      "Current iteration 79\n",
      "loss 0.2473 acc 0.9333\n",
      "Current iteration 80\n",
      "loss 0.1827 acc 0.9533\n",
      "Current iteration 81\n",
      "loss 0.1209 acc 0.9533\n",
      "Current iteration 82\n",
      "loss 0.2019 acc 0.9133\n",
      "Current iteration 83\n",
      "loss 0.1490 acc 0.9467\n",
      "Current iteration 84\n",
      "loss 0.1096 acc 0.9600\n",
      "Current iteration 85\n",
      "loss 0.1696 acc 0.9600\n",
      "Current iteration 86\n",
      "loss 0.1946 acc 0.9200\n",
      "Current iteration 87\n",
      "loss 0.1459 acc 0.9400\n",
      "Current iteration 88\n",
      "loss 0.1888 acc 0.9267\n",
      "Current iteration 89\n",
      "loss 0.1114 acc 0.9600\n",
      "Current iteration 90\n",
      "loss 0.1687 acc 0.9467\n",
      "Current iteration 91\n",
      "loss 0.1694 acc 0.9400\n",
      "Current iteration 92\n",
      "loss 0.2223 acc 0.9133\n",
      "Current iteration 93\n",
      "loss 0.1026 acc 0.9667\n",
      "Current iteration 94\n",
      "loss 0.1373 acc 0.9533\n",
      "Current iteration 95\n",
      "loss 0.1477 acc 0.9667\n",
      "Current iteration 96\n",
      "loss 0.2055 acc 0.9333\n",
      "Current iteration 97\n",
      "loss 0.1362 acc 0.9533\n",
      "Current iteration 98\n",
      "loss 0.0656 acc 0.9800\n",
      "Current iteration 99\n",
      "loss 0.1374 acc 0.9333\n",
      "Current iteration 100\n",
      "loss 0.1263 acc 0.9533\n",
      "Current iteration 101\n",
      "loss 0.2727 acc 0.9000\n",
      "Current iteration 102\n",
      "loss 0.1296 acc 0.9467\n",
      "Current iteration 103\n",
      "loss 0.0951 acc 0.9467\n",
      "Current iteration 104\n",
      "loss 0.1304 acc 0.9467\n",
      "Current iteration 105\n",
      "loss 0.1875 acc 0.9333\n",
      "Current iteration 106\n",
      "loss 0.1943 acc 0.9267\n",
      "Current iteration 107\n",
      "loss 0.1009 acc 0.9600\n",
      "Current iteration 108\n",
      "loss 0.1098 acc 0.9600\n",
      "Current iteration 109\n",
      "loss 0.1257 acc 0.9533\n",
      "Current iteration 110\n",
      "loss 0.0838 acc 0.9667\n",
      "Current iteration 111\n",
      "loss 0.1850 acc 0.9333\n",
      "Current iteration 112\n",
      "loss 0.1461 acc 0.9400\n",
      "Current iteration 113\n",
      "loss 0.0649 acc 0.9867\n",
      "Current iteration 114\n",
      "loss 0.1594 acc 0.9467\n",
      "Current iteration 115\n",
      "loss 0.1455 acc 0.9533\n",
      "Current iteration 116\n",
      "loss 0.1261 acc 0.9533\n",
      "Current iteration 117\n",
      "loss 0.1235 acc 0.9733\n",
      "Current iteration 118\n",
      "loss 0.0889 acc 0.9733\n",
      "Current iteration 119\n",
      "loss 0.1206 acc 0.9467\n",
      "Current iteration 120\n",
      "loss 0.1172 acc 0.9600\n",
      "Current iteration 121\n",
      "loss 0.1440 acc 0.9600\n",
      "Current iteration 122\n",
      "loss 0.0946 acc 0.9667\n",
      "Current iteration 123\n",
      "loss 0.0939 acc 0.9667\n",
      "Current iteration 124\n",
      "loss 0.2183 acc 0.9267\n",
      "Current iteration 125\n",
      "loss 0.0901 acc 0.9733\n",
      "Current iteration 126\n",
      "loss 0.1436 acc 0.9467\n",
      "Current iteration 127\n",
      "loss 0.0852 acc 0.9733\n",
      "Current iteration 128\n",
      "loss 0.1073 acc 0.9733\n",
      "Current iteration 129\n",
      "loss 0.1456 acc 0.9467\n",
      "Current iteration 130\n",
      "loss 0.1116 acc 0.9600\n",
      "Current iteration 131\n",
      "loss 0.1127 acc 0.9733\n",
      "Current iteration 132\n",
      "loss 0.0773 acc 0.9733\n",
      "Current iteration 133\n",
      "loss 0.0724 acc 0.9733\n",
      "Current iteration 134\n",
      "loss 0.1591 acc 0.9467\n",
      "Current iteration 135\n",
      "loss 0.1454 acc 0.9600\n",
      "Current iteration 136\n",
      "loss 0.1831 acc 0.9333\n",
      "Current iteration 137\n",
      "loss 0.1377 acc 0.9400\n",
      "Current iteration 138\n",
      "loss 0.1500 acc 0.9467\n",
      "Current iteration 139\n",
      "loss 0.0689 acc 0.9600\n",
      "Current iteration 140\n",
      "loss 0.0509 acc 0.9867\n",
      "Current iteration 141\n",
      "loss 0.1048 acc 0.9600\n",
      "Current iteration 142\n",
      "loss 0.0733 acc 0.9600\n",
      "Current iteration 143\n",
      "loss 0.0835 acc 0.9733\n",
      "Current iteration 144\n",
      "loss 0.0811 acc 0.9733\n",
      "Current iteration 145\n",
      "loss 0.1094 acc 0.9800\n",
      "Current iteration 146\n",
      "loss 0.1525 acc 0.9533\n",
      "Current iteration 147\n",
      "loss 0.0950 acc 0.9533\n",
      "Current iteration 148\n",
      "loss 0.1096 acc 0.9667\n",
      "Current iteration 149\n",
      "loss 0.0767 acc 0.9800\n",
      "Current iteration 150\n",
      "loss 0.1332 acc 0.9400\n",
      "Current iteration 151\n",
      "loss 0.0990 acc 0.9667\n",
      "Current iteration 152\n",
      "loss 0.0772 acc 0.9667\n",
      "Current iteration 153\n",
      "loss 0.0852 acc 0.9733\n",
      "Current iteration 154\n",
      "loss 0.0500 acc 0.9800\n",
      "Current iteration 155\n",
      "loss 0.0364 acc 0.9933\n",
      "Current iteration 156\n",
      "loss 0.0726 acc 0.9733\n",
      "Current iteration 157\n",
      "loss 0.0382 acc 0.9800\n",
      "Current iteration 158\n",
      "loss 0.1814 acc 0.9533\n",
      "Current iteration 159\n",
      "loss 0.1166 acc 0.9600\n",
      "Current iteration 160\n",
      "loss 0.0767 acc 0.9867\n",
      "Current iteration 161\n",
      "loss 0.2020 acc 0.9333\n",
      "Current iteration 162\n",
      "loss 0.0766 acc 0.9733\n",
      "Current iteration 163\n",
      "loss 0.0799 acc 0.9800\n",
      "Current iteration 164\n",
      "loss 0.0395 acc 0.9867\n",
      "Current iteration 165\n",
      "loss 0.0900 acc 0.9733\n",
      "Current iteration 166\n",
      "loss 0.1126 acc 0.9533\n",
      "Current iteration 167\n",
      "loss 0.0640 acc 0.9667\n",
      "Current iteration 168\n",
      "loss 0.0519 acc 0.9867\n",
      "Current iteration 169\n",
      "loss 0.0453 acc 0.9800\n",
      "Current iteration 170\n",
      "loss 0.1064 acc 0.9600\n",
      "Current iteration 171\n",
      "loss 0.0359 acc 0.9867\n",
      "Current iteration 172\n",
      "loss 0.0721 acc 0.9800\n",
      "Current iteration 173\n",
      "loss 0.0732 acc 0.9867\n",
      "Current iteration 174\n",
      "loss 0.0520 acc 0.9800\n",
      "Current iteration 175\n",
      "loss 0.0474 acc 0.9867\n",
      "Current iteration 176\n",
      "loss 0.1107 acc 0.9400\n",
      "Current iteration 177\n",
      "loss 0.0177 acc 0.9933\n",
      "Current iteration 178\n",
      "loss 0.0436 acc 0.9867\n",
      "Current iteration 179\n",
      "loss 0.0850 acc 0.9733\n",
      "Current iteration 180\n",
      "loss 0.0811 acc 0.9733\n",
      "Current iteration 181\n",
      "loss 0.1081 acc 0.9533\n",
      "Current iteration 182\n",
      "loss 0.1590 acc 0.9467\n",
      "Current iteration 183\n",
      "loss 0.0800 acc 0.9733\n",
      "Current iteration 184\n",
      "loss 0.0799 acc 0.9667\n",
      "Current iteration 185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1295 acc 0.9467\n",
      "Current iteration 186\n",
      "loss 0.1132 acc 0.9600\n",
      "Current iteration 187\n",
      "loss 0.0860 acc 0.9800\n",
      "Current iteration 188\n",
      "loss 0.0547 acc 0.9800\n",
      "Current iteration 189\n",
      "loss 0.1170 acc 0.9667\n",
      "Current iteration 190\n",
      "loss 0.0910 acc 0.9733\n",
      "Current iteration 191\n",
      "loss 0.0387 acc 0.9867\n",
      "Current iteration 192\n",
      "loss 0.0769 acc 0.9667\n",
      "Current iteration 193\n",
      "loss 0.0202 acc 0.9933\n",
      "Current iteration 194\n",
      "loss 0.0572 acc 0.9800\n",
      "Current iteration 195\n",
      "loss 0.0217 acc 1.0000\n",
      "Current iteration 196\n",
      "loss 0.0495 acc 0.9733\n",
      "Current iteration 197\n",
      "loss 0.0580 acc 0.9800\n",
      "Current iteration 198\n",
      "loss 0.0581 acc 0.9667\n",
      "Current iteration 199\n",
      "loss 0.0918 acc 0.9667\n",
      "Current iteration 200\n",
      "loss 0.0802 acc 0.9733\n",
      "TEST: loss 4.7665 acc 0.4920\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(model['init'])\n",
    "    for ind_epoch in range(0, num_epochs):\n",
    "        print('Current iteration {}'.format(ind_epoch + 1))\n",
    "        \n",
    "        for ind_ in range(0, int(50000 / batch_size)):\n",
    "            batch_X = x_train[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            batch_by = y_train[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            _, cur_loss, cur_acc = sess.run(\n",
    "                [model['opt'], model['loss'], model['acc']],\n",
    "                feed_dict={X: batch_X, by: batch_by, training :True})\n",
    "#             print(sess.run(model['preds'], feed_dict={X:x_train[:10], training:False}))\n",
    "#             if ind_ % num_display == 0:\n",
    "        print('loss {0:.4f} acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "    cur_acc_all = 0.0\n",
    "    cur_loss_all = 0.0\n",
    "    for ind_ in range(0, 10):\n",
    "        cur_loss, cur_acc = sess.run(\n",
    "                    [model['loss'], model['acc']],\n",
    "                    feed_dict={X: x_test[ind_*1000:(ind_+1)*1000], \n",
    "                               by: y_test[ind_*1000:(ind_+1)*1000],\n",
    "                              training : False})\n",
    "        cur_loss_all += cur_loss\n",
    "        cur_acc_all += cur_acc\n",
    "    print('TEST: loss {0:.4f} acc {1:.4f}'.format(cur_loss_all / 10.0, \n",
    "                                              cur_acc_all / 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
